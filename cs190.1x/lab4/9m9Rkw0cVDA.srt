0
00:00:00,000 --> 00:00:03,760


1
00:00:03,760 --> 00:00:05,780
In this segment, we'll provide an overview

2
00:00:05,780 --> 00:00:07,730
of the click-through rate prediction pipeline

3
00:00:07,730 --> 00:00:10,887
that you'll be working on in this week's Spark coding lab.

4
00:00:10,887 --> 00:00:13,220
The goal of the lab is to implement a click-through rate

5
00:00:13,220 --> 00:00:15,820
prediction pipeline using various techniques that we've

6
00:00:15,820 --> 00:00:18,710
discussed in this week's lecture.

7
00:00:18,710 --> 00:00:21,110
The raw data consists of a subset of a data

8
00:00:21,110 --> 00:00:24,380
from a Kaggle competition sponsored by Criteo.

9
00:00:24,380 --> 00:00:28,170
This data includes 39 features describing users, ads,

10
00:00:28,170 --> 00:00:29,850
and publishers.

11
00:00:29,850 --> 00:00:33,480
Many of these features contain a large number of categories.

12
00:00:33,480 --> 00:00:35,470
Additionally, for privacy purposes

13
00:00:35,470 --> 00:00:37,800
the features have been anonymized.

14
00:00:37,800 --> 00:00:40,580
We'll be using a small fraction of the Kaggle data.

15
00:00:40,580 --> 00:00:43,280
And the Kaggle data itself is only a small fraction

16
00:00:43,280 --> 00:00:46,280
of Criteo's actual data.

17
00:00:46,280 --> 00:00:48,410
As on the previous lab, you'll split

18
00:00:48,410 --> 00:00:53,730
this data set into training, validation, and test data sets.

19
00:00:53,730 --> 00:00:55,350
You'll next need to extract features

20
00:00:55,350 --> 00:00:57,530
to feed into a supervised learning model.

21
00:00:57,530 --> 00:01:00,810
And feature extraction is the main focus of this lab.

22
00:01:00,810 --> 00:01:04,080
You'll create OHE features, as well as hashed features,

23
00:01:04,080 --> 00:01:07,550
and store these features using a sparse representation.

24
00:01:07,550 --> 00:01:09,900
You will also visualize feature frequency

25
00:01:09,900 --> 00:01:14,520
to get a better sense of the sparsity pattern of this data.

26
00:01:14,520 --> 00:01:18,340
Given a set of features, either OHE or hashed features,

27
00:01:18,340 --> 00:01:22,720
you will use MLlib to train logistic regression models.

28
00:01:22,720 --> 00:01:24,310
You will then perform Hyperparameter

29
00:01:24,310 --> 00:01:27,370
tuning to search for a good regularization parameter,

30
00:01:27,370 --> 00:01:29,930
evaluating the results via log loss,

31
00:01:29,930 --> 00:01:32,720
and visualizing the results of your grid search.

32
00:01:32,720 --> 00:01:34,800
You'll also look at a ROC plot to understand

33
00:01:34,800 --> 00:01:36,650
the trade-off between the false positive

34
00:01:36,650 --> 00:01:39,920
and the true positive rates.

35
00:01:39,920 --> 00:01:43,240
You will then evaluate the final model, again, using log loss

36
00:01:43,240 --> 00:01:46,630
and compare its accuracy to that of a simple baseline that

37
00:01:46,630 --> 00:01:49,020
always predicts the fraction of training points

38
00:01:49,020 --> 00:01:51,974
that correspond to click-through events.

39
00:01:51,974 --> 00:01:53,390
You will also compare the accuracy

40
00:01:53,390 --> 00:01:57,980
of models train using OHE features and hashed features.

41
00:01:57,980 --> 00:01:59,510
The final model you train could be

42
00:01:59,510 --> 00:02:02,467
used for feature click-through rate prediction.

43
00:02:02,467 --> 00:02:02,967


